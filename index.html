<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Isha Chaudhary</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="http://placeholder-url.com" />
	    <meta property="og:title" content="Isha Chaudhary" />
	    <meta property="og:image" content="img/profile_pic.jpeg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Isha Chaudhary">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" crossorigin="anonymous">
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
    </head>
    <body>
        <!-- Table of Contents Sidebar -->
        <div class="toc-sidebar">
            <!-- Profile Section -->
            <div class="toc-profile">
                <img class="toc-profile-pic" src="img/profile_pic.jpeg" alt="Isha Chaudhary">
                <h3 class="toc-name">Isha Chaudhary</h3>
                <div class="toc-contact">
                    <div class="toc-email">
                        <i class="fa fa-envelope"></i> isha4@illinois.edu
                    </div>
                    <div class="toc-institution">
                        <i class="fa fa-building"></i> Siebel School of Computing and Data Science, UIUC
                    </div>
                </div>
            </div>
            
            <!-- Navigation Links -->
            <ul class="toc-list">
                <li class="toc-item"><a href="#about" class="toc-link">About</a></li>
                <li class="toc-item"><a href="#news" class="toc-link">News</a></li>
                <li class="toc-item"><a href="#publications" class="toc-link">Publications</a></li>
                <li class="toc-item"><a href="#affiliations" class="toc-link">Work Experience</a></li>
            </ul>
        </div>
        
        <div class="container mt-4 mb-5">
            <div class="row mb-4" id="about">
                <div class="col">
                    <h1>Isha Chaudhary</h1>
                    <p>
                        Hi! I'm Isha Chaudhary, a Ph.D. student in Computer Science at the University of Illinois Urbana-Champaign. I'm fortunate to be advised by <a href="https://ggndpsngh.github.io/" target="_blank">Prof. Gagandeep Singh</a>. I completed my B.Tech. in Electrical Engineering from IIT Delhi, where I was awarded the Institute Silver Medal and Prof. C.S. Jha Memorial Award. I earned an MS degree in CS from UIUC. 
                    </p>
                    <p>
                        I am interested in Trustworthy AI systems. These days I work on making Large Frontier Models trustworthy using formal methods. 
                        Please feel free to reach out if you have any interesting ideas in 
                        this area on which we could collaborate. For the most up-to-date 
                        information about me, please check out my CV.
                    </p>
                    <p class="mt-3">
                        <a href="files/Isha_Research_CV.pdf" target="_blank">CV</a> &middot;
                        <a href="https://scholar.google.com/citations?hl=en&user=mC2cu8QAAAAJ" target="_blank">Google Scholar</a> &middot;
                        <a href="https://www.linkedin.com/in/isha-chaudhary-36242317b/" target="_blank">LinkedIn</a> &middot;
                        <a href="https://github.com/ishcha" target="_blank">Github</a> &middot;
                        <a href="https://x.com/Ish_cha_" target="_blank">Twitter</a>
                    </p>
                </div>
            </div>
            <div class="row mt-5" id="news">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul style="list-style: none; padding-left: 0;">
                        <li class="mb-3">
                            <strong>Jan 2026</strong> &mdash; Hat-trick of paper acceptances: 
                            <a href="https://lnkd.in/gZ6R8xDN">LLMCert-C</a> (AISTATS Spotlight), 
                            <a href="https://lnkd.in/gam_rAVV">QRLLM</a> (ICLR 2026), and 
                            <a href="https://lnkd.in/gUUeGRaY">SpecTRA</a> (ICSE 2026 Poster) — advancing LLM certification, AI safety, and specification generation.
                        </li>                        
                        <li class="mb-3">
                            <strong>May 2025</strong> &mdash; Received an MS degree in CS from UIUC.
                        </li>
                        <li class="mb-3">
                            <strong>Apr 2025</strong> &mdash; Project page of LLMCert is live at <a href="https://certifyllm.com/" target="_blank">https://certifyllm.com/</a>.
                        </li>
                    </ul>
                    <div class="collapse" id="olderNews">
                        <ul style="list-style: none; padding-left: 0;">
                            <li class="mb-3">
                            <strong>Feb 2025</strong> &mdash; Honored to receive the CSL student conference 2025 Best Presentation Award in the Security and Privacy track.
                        </li>
                        <li class="mb-3">
                            <strong>Jan 2025</strong> &mdash; LLMCert-B is accepted at ICLR 2025.
                        </li>
                    
                            <li class="mb-3">
                                <strong>Mar 2024</strong> &mdash; Passed my oral Qualifying Exam.
                            </li>
                            <li class="mb-3">
                                <strong>Mar 2024</strong> &mdash; LLMCert-C got accepted at SET-LLM@ICLR 2024.
                            </li>
                            <li class="mb-3">
                                <strong>Feb 2024</strong> &mdash; COMET got accepted at MLSys 2024.
                            </li>
                            <li class="mb-3">
                                <strong>Feb 2024</strong> &mdash; Our paper on Priming Attacks on LLMs is now an ICLR 2024 Tiny paper.
                            </li>
                            <li class="mb-3">
                                <strong>May 2023</strong> &mdash; Thanks to ICML 2023 for awarding me a travel grant.
                            </li>
                            <li class="mb-3">
                                <strong>May 2023</strong> &mdash; Thanks to ACM SIGPLAN for selecting me for PLMW@PLDI 2023.
                            </li>
                            <li class="mb-3">
                                <strong>Feb 2023</strong> &mdash; Thanks to NSF for awarding a travel grant for IEEE SaTML 2023.
                            </li>
                            <li class="mb-3">
                                <strong>Nov 2022</strong> &mdash; Thanks to IIT Delhi for awarding me the Institute Silver Medal and Prof. C.S. Jha Memorial Excellence Award at Convocation 2022. Deeply honored to receive the prestigious awards.
                            </li>
                            <li class="mb-3">
                                <strong>Aug 2022</strong> &mdash; I have joined as a Ph.D. student in Computer Science at the University of Illinois, Urbana-Champaign.
                            </li>
                        </ul>
                    </div>
                    <button class="btn btn-link pl-0" type="button" data-toggle="collapse" data-target="#olderNews" aria-expanded="false" aria-controls="olderNews">
                        <span class="show-more">Show more news <i class="fa fa-chevron-down"></i></span>
                        <span class="show-less" style="display:none;">Show less news <i class="fa fa-chevron-up"></i></span>
                    </button>
                    <script>
                        document.addEventListener('DOMContentLoaded', function() {
                            const olderNews = document.getElementById('olderNews');
                            const showMore = document.querySelector('.show-more');
                            const showLess = document.querySelector('.show-less');
                            
                            if (olderNews && showMore && showLess) {
                                olderNews.addEventListener('show.bs.collapse', function () {
                                    showMore.style.display = 'none';
                                    showLess.style.display = 'inline';
                                });
                                olderNews.addEventListener('hide.bs.collapse', function () {
                                    showMore.style.display = 'inline';
                                    showLess.style.display = 'none';
                                });
                            }
                        });
                    </script>
                </div>
            </div>
            <hr>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>
                    <ul class="pl">
                        <li>
                            <b>Lumos: Let there be Language Model System Certification</b>
                            <br/>
                            <b>Isha Chaudhary</b>, Vedaant Jain, Avaljot Singh, Kavya Sachdeva, Sayan Ranu, Gagandeep Singh.
                            <br/>
                            ArXiv, 2025.
                            <br/>
                            [<a href="files/lumos.pdf" target="_blank">paper</a>] [<a href="files/lumos.bib" target="_blank">bib</a>] [<a href="#" onclick="toggleElement('lumos_abstract'); return false;">abstract</a>] [<a href="https://github.com/uiuc-focal-lab/Lumos" target="_blank">code</a>]
                            <div id="lumos_abstract" class="abstract" style="display:none;">
                                <p>
                                    We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Certifying Counterfactual Bias in LLMs</b>
                            <br/>
                            <b>Isha Chaudhary</b>, Qian Hu, Manoj Kumar, Morteza Ziyadi, Rahul Gupta, Gagandeep Singh.
                            <br/>
                            <b class="conference">ICLR</b> 2025.
                            <br/>
                            [<a href="files/llmcert-b.pdf" target="_blank">paper</a>] [<a href="files/bias_cert.bib" target="_blank">bib</a>] [<a href="#" onclick="toggleElement('bias_cert_abstract'); return false;">abstract</a>] [<a href="https://github.com/uiuc-focal-lab/LLMCert-B" target="_blank">code</a>]
                            <div id="bias_cert_abstract" class="abstract" style="display:none;">
                                <p>
                                    Large Language Models (LLMs) can produce biased responses that can cause representational harms. However, conventional studies are insufficient to thoroughly evaluate biases across LLM responses for different demographic groups (a.k.a. counterfactual bias), as they do not scale to large number of inputs and do not provide guarantees. Therefore, we propose the first framework, LLMCert-B that certifies LLMs for counterfactual bias on distributions of prompts. A certificate consists of high-confidence bounds on the probability of unbiased LLM responses for any set of counterfactual prompts - prompts differing by demographic groups, sampled from a distribution. We illustrate counterfactual bias certification for distributions of counterfactual prompts created by applying prefixes sampled from prefix distributions, to a given set of prompts. We consider prefix distributions consisting random token sequences, mixtures of manual jailbreaks, and perturbations of jailbreaks in LLM's embedding space. We generate non-trivial certificates for SOTA LLMs, exposing their vulnerabilities over distributions of prompts generated from computationally inexpensive prefix distributions.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Certifying Reading Comprehension in Large Language Models</b>
                            <br/>
                            <b>Isha Chaudhary</b>, Vedaant Jain, Gagandeep Singh.
                            <br/>
                            <b class="conference">AISTATS 2026</b> (Spotlight); also in SeT LLM Workshop @ ICLR 2024.
                            <br/>
                            [<a href="files/LLMCert_C_AISTATS (3).pdf" target="_blank">paper</a>] [<a href="#" onclick="toggleElement('quacerc_abstract'); return false;">abstract</a>] [<a href="https://github.com/uiuc-focal-lab/LLMCert-C" target="_blank">code</a>]
                            <div id="quacerc_abstract" class="abstract" style="display:none;">
                                <p>
                                    Large Language Models (LLMs) are increasingly deployed in safety-critical systems
that rely heavily on reading comprehension—extracting and reasoning over extensive in-context information. However, existing evaluations of LLMs on reading comprehension are typically over limited test sets
containing only a tiny fraction of the vast
number of possible prompts. Empirical evaluations on these test sets have questionable
reliability and generalizability. We propose
a fundamentally different approach: rather
than evaluating LLMs with fixed datasets,
we introduce the first framework for certifying LLMs based on large probability distributions over realistic reading comprehension
prompts. To create these distributions, we
use knowledge graphs (KGs) as structured
representations of real-world knowledge and
define the distributions’ sample spaces with
prompts based on directed acyclic subgraphs
of the KGs. We also incorporate realistic
noise designed to mimic real-world complexity, such as distractor texts and synonyms.
Our prompt distributions have i.i.d. samplers represented as probabilistic programs.
Our framework generates novel, formal probabilistic quantitative certificates that provide
high-confidence, tight bounds on the probability that an LLM correctly answers any
prompt drawn from these distributions. We
enable formal certification for SOTA LLMs
by using an input-output example-driven approach. We apply our framework to certify
SOTA LLMs in precision medicine and general question-answering domains. Our results
uncover previously unknown vulnerabilities
caused by natural prompt noise and establish the first formal performance hierarchies
among these models.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Quantifying Risks in Multi-turn Conversation with Large Language Models</b>
                            <br/>
                            Chengxiao Wang, <b>Isha Chaudhary</b>, Qian Hu, Weitong Ruan, Rahul Gupta, Gagandeep Singh.
                            <br/>
                            <b class="conference">ICLR</b> 2026.
                            <br/>
                            [<a href="files/qrllm.pdf" target="_blank">paper</a>] [<a href="files/multi_turn.bib" target="_blank">bib</a>] [<a href="#" onclick="toggleElement('multi_turn_abstract'); return false;">abstract</a>] 
                            <div id="multi_turn_abstract" class="abstract" style="display:none;">
                                <p>
                                    Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations. In this work, we propose QRLLM, a novel, principled Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions: random node, graph path, adaptive with rejection. Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70\% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>COMET: X86 Cost Model Explanation Framework</b>
                            <br/>
                            <b>Isha Chaudhary</b>, Alex Renda, Charith Mendis, Gagandeep Singh.
                            <br/>
                            <b class="conference">MLSys</b> 2024; also in XAI-in-Action Workshop @ NeurIPS 2023.
                            <br/>
                            [<a href="files/comet.pdf" target="_blank">paper</a>] [<a href="files/comet.bib" target="_blank">bib</a>] [<a href="#" onclick="toggleElement('comet_abstract'); return false;">abstract</a>] [<a href="https://github.com/uiuc-focal-lab/COMET" target="_blank">code</a>]
                            <div id="comet_abstract" class="abstract" style="display:none;">
                                <p>
                                    Cost models predict the cost of executing given assembly code basic blocks on a specific microarchitecture. Recently, neural cost models have been shown to be fairly accurate and easy to construct. They can replace heavily engineered analytical cost models used in mainstream compiler workflows. However, their black-box nature discourages their adoption. In this work, we develop the first framework, COMET, for generating faithful, generalizable, and intuitive explanations for neural cost models. We generate and compare COMET’s explanations for the popular neural cost model, Ithemal against those for an accurate CPU simulation-based cost model, uiCA. Our empirical findings show an inverse correlation between the prediction errors of Ithemal and uiCA and the granularity of basic block features in COMET’s explanations for them, thus indicating potential reasons for the higher error of Ithemal with respect to uiCA.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Specification Generation for Neural Networks in Systems</b>
                            <br/>
                            <b>Isha Chaudhary</b>, Shuyi Lin, Cheng Tan, Gagandeep Singh.
                            <br/>
                            <b class="conference">ICSE 2026</b> (Poster); also in ML4Wireless Workshop @ ICML 2025.
                            <br/>
                            [<a href="files/spectra.pdf" target="_blank">paper</a>] [<a href="files/spectra.bib" target="_blank">bib</a>] [<a href="#" onclick="toggleElement('spectra_abstract'); return false;">abstract</a>] [<a href="https://github.com/uiuc-focal-lab/spectra" target="_blank">code</a>]
                            <div id="spectra_abstract" class="abstract" style="display:none;">
                                <p>
                                    Specifications of desirable behaviors are crucial to test and verify the
trustworthiness of computer systems, especially important as neural networks replace system components. Specifications are hand-
crafted by domain experts and are thus error-prone and difficult
to scale across diverse neural-network–based system applications.
Traditional, well-tested algorithms that neural networks replace,
encode domain knowledge about correct behavior. We leverage this
to propose SpecTRA, an automated framework that generates specifications for neural networks using behavioral observations from
traditional algorithms. SpecTRA formulates specification generation as an optimization problem and solves it by clustering observed
tion as an optimization problem and solves it by clustering observed traditional behaviors into compact specifications. We demonstrate
the correctness and utility of SpecTRA’s specifications for testing
and verifying neural networks in adaptive bitrate and congestion
control, showing that they align with intuition and reveal vulnerabilities in SOTA models for computer systems.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <b>Bypassing the Safety Training of Open-Source LLMs with Priming Attacks</b>
                            <br/>
                            Jason Vega*, <b>Isha Chaudhary</b>*, Changming Xu*, Gagandeep Singh (* equal contribution).
                            <br/>
                            Tiny Papers @ ICLR, 2024.
                            <br/>
                            [<a href="files/priming.pdf" target="_blank">paper</a>] [<a href="files/priming.bib" target="_blank">bib</a>] [<a href="#" onclick="toggleElement('priming_abstract'); return false;">abstract</a>] [<a href="https://github.com/uiuc-focal-lab/llm-priming-attacks" target="_blank">code</a>]
                            <div id="priming_abstract" class="abstract" style="display:none;">
                                <p>
                                    With the recent surge in popularity of LLMs has come an ever-increasing need for LLM safety training. In this paper, we investigate the fragility of SOTA open-source LLMs under simple, optimization-free attacks we refer to as priming attacks, which are easy to execute and effectively bypass alignment from safety training. Our proposed attack improves the Attack Success Rate on Harmful Behaviors, as measured by Llama Guard, by up to 3.3× compared to baselines.
                                </p>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="affiliations">
                <div class="col">
                    <h2>Work Experience</h2>
                    <div class="work-experience-container">
                        <div class="row">
                            <!-- Adobe -->
                            <div class="col-md-3 col-sm-6 mb-4">
                                <div class="work-experience-card">
                                    <div class="work-logo">
                                        <a href="https://www.adobe.com" target="_blank">
                                            <img src="https://upload.wikimedia.org/wikipedia/commons/8/8d/Adobe_Corporate_Logo.png" alt="Adobe">
                                        </a>
                                    </div>
                                    <div class="work-info">
                                        <h4>Adobe</h4>
                                        <p>Research Intern</p>
                                        <small>May - Aug 2021</small>
                                    </div>
                                </div>
                            </div>

                            <!-- Microsoft AI -->
                            <div class="col-md-3 col-sm-6 mb-4">
                                <div class="work-experience-card">
                                    <div class="work-logo">
                                        <a href="https://www.microsoft.com/en-us/ai" target="_blank">
                                            <img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg" alt="Microsoft AI">
                                        </a>
                                    </div>
                                    <div class="work-info">
                                        <h4>Microsoft AI</h4>
                                        <p>Data Science Intern</p>
                                        <small>May - Aug 2024</small>
                                    </div>
                                </div>
                            </div>

                            <!-- Microsoft Research -->
                            <div class="col-md-3 col-sm-6 mb-4">
                                <div class="work-experience-card">
                                    <div class="work-logo">
                                        <a href="https://www.microsoft.com/en-us/research/" target="_blank">
                                            <img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg" alt="Microsoft Research">
                                        </a>
                                    </div>
                                    <div class="work-info">
                                        <h4>Microsoft Research</h4>
                                        <p>Research Intern</p>
                                        <small>May - Aug 2025</small>
                                    </div>
                                </div>
                            </div>

                            <!-- AWS -->
                            <div class="col-md-3 col-sm-6 mb-4">
                                <div class="work-experience-card">
                                    <div class="work-logo">
                                        <a href="https://aws.amazon.com/" target="_blank">
                                            <img src="https://upload.wikimedia.org/wikipedia/commons/9/93/Amazon_Web_Services_Logo.svg" alt="AWS">
                                        </a>
                                    </div>
                                    <div class="work-info">
                                        <h4>Amazon Web Services</h4>
                                        <p>Applied Science Intern</p>
                                        <small>Sep - Dec 2025</small>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <hr>
            <div class="row" id="miscellany">
                <div class="col">
                    <h2>Miscellany</h2>
                    <ul>
                        <li>
                            Feel free to
                            use <a href="https://github.com/nelson-liu/website"
                            target="_blank">this website's source code</a>, I'd
                            just appreciate if you linked back
                            to <a href="https://nelsonliu.me/"
                            target="_blank">Nelson Liu's original page</a>.
                        </li>
                    </ul>
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            © Isha Chaudhary, 2026
                        </p>
                    </div>
                    <div class="col-6 col-md text-right">
                        <!-- Institution logos can be added here if desired -->
                    </div>
                </div>
            </footer>
        </div>
        
        <!-- Table of Contents JavaScript -->
        <script>
            // Toggle function for abstracts and bibtex
            function toggleElement(elementId) {
                const element = document.getElementById(elementId);
                if (element) {
                    element.style.display = element.style.display === 'none' ? 'block' : 'none';
                }
            }
            
            document.addEventListener('DOMContentLoaded', function() {
                // Smooth scrolling for TOC links
                const tocLinks = document.querySelectorAll('.toc-link');
                
                tocLinks.forEach(function(link) {
                    link.addEventListener('click', function(e) {
                        e.preventDefault();
                        const targetId = this.getAttribute('href');
                        const targetElement = document.querySelector(targetId);
                        
                        console.log('TOC click:', targetId, targetElement); // Debug log
                        
                        if (targetElement) {
                            const offsetTop = targetElement.offsetTop - 100;
                            window.scrollTo({
                                top: offsetTop,
                                behavior: 'smooth'
                            });
                        }
                    });
                });
                
                // Highlight current section on scroll
                function updateActiveSection() {
                    const scrollPos = window.pageYOffset + 200;
                    const sections = ['#about', '#news', '#publications', '#affiliations'];
                    
                    // Remove active class from all links
                    tocLinks.forEach(function(link) {
                        link.classList.remove('active');
                    });
                    
                    // Find current section and highlight it
                    for (let i = sections.length - 1; i >= 0; i--) {
                        const section = document.querySelector(sections[i]);
                        if (section && section.offsetTop <= scrollPos) {
                            const activeLink = document.querySelector('.toc-link[href="' + sections[i] + '"]');
                            if (activeLink) {
                                activeLink.classList.add('active');
                            }
                            break;
                        }
                    }
                }
                
                // Update on scroll
                window.addEventListener('scroll', updateActiveSection);
                
                // Update on page load
                updateActiveSection();
            });
        </script>
    </body>
</html>